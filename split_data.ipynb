{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a706cc2-02a1-4382-ae74-61038323caa5",
   "metadata": {},
   "source": [
    "# Data split version description\n",
    "## Version 1\n",
    ">>\n",
    "가장 기본적인 random sampling (비복원 추출) 방식을 따랐습니다.\\\n",
    "Random seed는 42로 설정했습니다.\\\n",
    "생성되는 .json 파일은 ufo 폴더 안에 위치하도록 했으며 dataset 별로 모두 ufo 폴더가 있다고 가정하고 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c241112-eb8b-457f-8002-58443bc87532",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f1c525-6820-4739-bd00-40f0c6a17d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9973a1-5b4e-4f36-a19b-5f52be25d99d",
   "metadata": {},
   "source": [
    "## Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8be1ae-d453-4cb9-8fa3-ceb51cacba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559fd68-cc29-490e-af8a-00dfa62bc8b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169d1222-c1fa-4899-b26e-ec674f3a4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with Path(filename).open(encoding='utf8') as handle:\n",
    "        ann = json.load(handle)\n",
    "    return ann    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bf9e2a-6d8a-4b7d-9d60-e911c83bbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../input/data/'\n",
    "path_ICDAR17 = 'ICDAR17_Korean/ufo/'\n",
    "data_nm = 'train.json'\n",
    "\n",
    "data = read_json(os.path.join(root_dir, path_ICDAR17, data_nm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de625613-4351-4419-b6d5-88f4ac859b24",
   "metadata": {},
   "source": [
    "# Get image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be58ccda-4791-4163-b754-e86e44108893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_size(quads):\n",
    "    \"\"\" 단어 영역의 사각형 좌표가 주어졌을 때 가로, 세로길이를 계산해주는 함수.\n",
    "    TODO: 각 변의 길이를 단순히 max로 처리하기때문에 직사각형에 가까운 형태가 아니면 약간 왜곡이 있다.\n",
    "    Args:\n",
    "        quads: np.ndarray(n, 4, 2) n개 단어 bounding-box의 4개 점 좌표 (단위 pixel)\n",
    "    Return:\n",
    "        sizes: np.ndarray(n, 2) n개 box의 (height, width)쌍\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    for i, j in [(1, 2), (3, 0), (0, 1), (2, 3)]: # [right(height), left(height), upper(width), lower(width)] sides\n",
    "        dists.append(np.linalg.norm(quads[:, i] - quads[:, j], ord=2, axis=1))\n",
    "\n",
    "    dists = np.stack(dists, axis=-1).reshape(-1, 2, 2) # shape (n, 2, 2) widths, heights into separate dim\n",
    "    return np.rint(dists.mean(axis=-1)).astype(int)\n",
    "\n",
    "\n",
    "def rectify_poly(poly, direction, img_w, img_h):\n",
    "    \"\"\"일반 polygon형태인 라벨을 크롭하고 rectify해주는 함수.\n",
    "    Args:\n",
    "        poly: np.ndarray(2n+4, 2) (where n>0), 4, 6, 8\n",
    "        image: np.ndarray opencv 포멧의 이미지\n",
    "        direction: 글자의 읽는 방향과 진행 방향의 수평(Horizontal) 혹은 수직(Vertical) 여부\n",
    "    Return:\n",
    "        rectified: np.ndarray(2, ?) rectify된 단어 bbox의 사이즈.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_pts = poly.shape[0]\n",
    "    assert n_pts % 2 == 0\n",
    "    if n_pts == 4:\n",
    "        size = get_box_size(poly[None])\n",
    "        h = size[:, 0] / img_h\n",
    "        w = size[:, 1] / img_w\n",
    "        return np.stack((h,w))\n",
    "\n",
    "    def unroll(indices):\n",
    "        return list(zip(indices[:-1], indices[1:]))\n",
    "\n",
    "    # polygon하나를 인접한 사각형 여러개로 쪼갠다.\n",
    "    indices = list(range(n_pts))\n",
    "    if direction == 'Horizontal':\n",
    "        upper_pts = unroll(indices[:n_pts // 2]) # (0, 1), (1, 2), ... (4, 5)\n",
    "        lower_pts = unroll(indices[n_pts // 2:])[::-1] # (8, 9), (7, 8), ... (6, 7)\n",
    "\n",
    "        quads = np.stack([poly[[i, j, k, l]] for (i, j), (k, l) in zip(upper_pts, lower_pts)])\n",
    "    else:\n",
    "        right_pts = unroll(indices[1:n_pts // 2 + 1]) # (1, 2), (2, 3), ... (4, 5)\n",
    "        left_pts = unroll([0] + indices[:n_pts // 2:-1]) # (0, 9), (9, 8), ... (7, 6)\n",
    "\n",
    "        quads = np.stack([poly[[i, j, k, l]] for (j, k), (i, l) in zip(right_pts, left_pts)])\n",
    "\n",
    "    sizes = get_box_size(quads)\n",
    "    if direction == 'Horizontal':\n",
    "        h = sizes[:, 0].max() / img_h\n",
    "        widths = sizes[:, 1]\n",
    "        w = np.sum(widths) / img_w\n",
    "        return np.stack((h,w)).reshape(2,-1)\n",
    "        #return np.stack((h,w))\n",
    "    elif direction == 'Vertical':\n",
    "        heights = sizes[:, 0]\n",
    "        w = sizes[:, 1].max() / img_w\n",
    "        h = np.sum(heights) / img_h\n",
    "        return np.stack((h,w)).reshape(2,-1)\n",
    "    else:\n",
    "        h = sizes[:, 0] / img_h\n",
    "        w = sizes[:, 1] / img_w\n",
    "        return np.stack((h,w),-1)\n",
    "    \n",
    "def get_image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6a8053-48db-42a6-ae5d-bc2c37473d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "df['image'] = []\n",
    "df['word_counts'] = []\n",
    "df['image_width'] = []\n",
    "df['image_height'] = []\n",
    "df['image_tags'] = []\n",
    "img_tags = []\n",
    "\n",
    "quads = []\n",
    "polys = []\n",
    "seq_length = []\n",
    "hor_sizes = []\n",
    "ver_sizes = []\n",
    "irr_sizes = []\n",
    "languages = []\n",
    "orientation = []\n",
    "word_tags = []\n",
    "aspect_ratio = []\n",
    "ver_string = []\n",
    "\n",
    "for image_key, image_value in data[\"images\"].items():\n",
    "    df['image'].append(image_key)\n",
    "    img_w = image_value['img_w']\n",
    "    img_h = image_value['img_h']\n",
    "    df['image_width'].append(img_w)\n",
    "    df['image_height'].append(img_h)\n",
    "    df['image_tags'].append(image_value['tags'])\n",
    "    df['image_tags']= [['None'] if v is None else v for v in df['image_tags']] # our data does not inlcude multi-tag images \n",
    "    word_ann = image_value['words']\n",
    "    count_ill = 0 \n",
    "    for word in word_ann.values():\n",
    "        if word['illegibility']== False:\n",
    "            orientation.append(word['orientation'])\n",
    "            orientation = [v for v in orientation]\n",
    "            seq_length.append(len(word['transcription']))\n",
    "            languages.append(word['language'])\n",
    "            languages = [['None'] if v is None else v for v in languages] # our data does not inlcude multi-language words\n",
    "            if word['word_tags'] != None:\n",
    "                word_tags.extend(word['word_tags'][:])\n",
    "            elif word['word_tags']== None:\n",
    "                word_tags.append('None')\n",
    "            poly = np.int32(word['points'])\n",
    "            size = rectify_poly(poly, word['orientation'], img_w, img_h)\n",
    "            if word['orientation'] == 'Horizontal':\n",
    "                hor_sizes.append(size)\n",
    "            elif word['orientation'] == 'Vertical':\n",
    "                ver_sizes.append(size)\n",
    "            else:\n",
    "                irr_sizes.append(size)\n",
    "            \n",
    "        else:\n",
    "            count_ill += 1\n",
    "        break\n",
    "    \n",
    "    df['word_counts'].append(len(word_ann)-count_ill)\n",
    "    \n",
    "    \n",
    "all_sizes = hor_sizes + ver_sizes + irr_sizes\n",
    "quad_area = [all_sizes[i][0]*all_sizes[i][1] for i in range(len(all_sizes))]\n",
    "total_area = []\n",
    "for s in quad_area:\n",
    "    if s.shape[0] == 1:\n",
    "        total_area.append(np.sum(s[0])) \n",
    "    else:\n",
    "        total_area.append(np.sum(s))\n",
    "\n",
    "hor_aspect_ratio = [hor_sizes[i][1]/hor_sizes[i][0] for i in range(len(hor_sizes))]\n",
    "ver_aspect_ratio = [ver_sizes[i][1]/ver_sizes[i][0] for i in range(len(ver_sizes))]\n",
    "\n",
    "image_df = pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6299ddbd-fec0-4e45-8d00-2b1144cf7894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>image_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_4380.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1836</td>\n",
       "      <td>2448</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_4583.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>2268</td>\n",
       "      <td>2268</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_4234.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>2592</td>\n",
       "      <td>3456</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_4345.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>1836</td>\n",
       "      <td>2448</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_4016.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1836</td>\n",
       "      <td>2448</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>img_1048.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>3024</td>\n",
       "      <td>2268</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>img_1071.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>2448</td>\n",
       "      <td>1836</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>img_1122.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>2448</td>\n",
       "      <td>1836</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>img_1131.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>3144</td>\n",
       "      <td>2328</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>img_1100.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>2448</td>\n",
       "      <td>1836</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image  word_counts  image_width  image_height image_tags\n",
       "0    img_4380.jpg            3         1836          2448     [None]\n",
       "1    img_4583.jpg            2         2268          2268     [None]\n",
       "2    img_4234.jpg            7         2592          3456     [None]\n",
       "3    img_4345.jpg            7         1836          2448     [None]\n",
       "4    img_4016.jpg            1         1836          2448     [None]\n",
       "..            ...          ...          ...           ...        ...\n",
       "531  img_1048.jpg            4         3024          2268     [None]\n",
       "532  img_1071.jpg            2         2448          1836     [None]\n",
       "533  img_1122.jpg            9         2448          1836     [None]\n",
       "534  img_1131.jpg            2         3144          2328     [None]\n",
       "535  img_1100.jpg            2         2448          1836     [None]\n",
       "\n",
       "[536 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data frame\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80af8d89-a736-4c64-bd76-c25882c10137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split images at random\n",
    "X_train_v1, X_valid_v1, y_train_v1, y_valid_v1 = train_test_split(image_df.image, image_df.image, test_size=0.2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f3cc598-dd45-45e6-b3ea-004af6d5fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# Simple check split result\n",
    "print(sum(X_train_v1!=y_train_v1))\n",
    "print(set(y_train_v1).intersection(set(y_valid_v1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366e7ada-6ef1-4e65-a27f-82deebc180bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6054358-60be-4f9e-a82d-1f8ad9890973",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v1 = {'images': {k: v for k, v in data['images'].items() if k in X_train_v1.values}}\n",
    "valid_v1 = {'images': {k: v for k, v in data['images'].items() if k in X_valid_v1.values}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25067fe1-4040-424e-99d7-8d03dceb2288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "# Just for check\n",
    "print(len(train_v1['images']))\n",
    "print(len(valid_v1['images']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d8144-d458-4c25-80b9-c23cefcb5541",
   "metadata": {},
   "source": [
    "# Save validation version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb829f68-1469-4786-ad3a-829ba3304e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data: dict, file_nm: str, dir_path: str):\n",
    "    with open(os.path.join(dir_path, file_nm), 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c58c0d63-7b24-4435-b599-027512f59297",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [train_v1,\n",
    "             valid_v1\n",
    "            ]\n",
    "file_nm_list = ['train_v1.json',\n",
    "                'valid_v1.json',\n",
    "               ]\n",
    "\n",
    "for data, file_nm in zip(data_list, file_nm_list):\n",
    "    save_json(data, file_nm, dir_path=os.path.join(root_dir, path_ICDAR17))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

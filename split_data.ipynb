{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a706cc2-02a1-4382-ae74-61038323caa5",
   "metadata": {},
   "source": [
    "# Data split version description\n",
    "## Version 1\n",
    ">>\n",
    "가장 기본적인 random sampling (비복원 추출) 방식을 따랐습니다.\\\n",
    "Random seed는 42로 설정했습니다.\\\n",
    "생성되는 .json 파일은 ufo 폴더 안에 위치하도록 했으며 dataset 별로 모두 ufo 폴더가 있다고 가정하고 진행합니다.\n",
    "\n",
    "## Version 2\n",
    ">>\n",
    "Aspect ratio를 기준으로 StratifiedGroupKFold를 적용했습니다.\\\n",
    "Random seed는 42로 설정했습니다.\\\n",
    "생성되는 .json 파일은 ufo 폴더 안에 위치하도록 했으며 dataset 별로 모두 ufo 폴더가 있다고 가정하고 진행합니다.\\\n",
    "image key를 group으로 설정하고, auto_binning 함수를 이용해 aspect ratio를 다수의 class로 근사해 Stratify 했습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c241112-eb8b-457f-8002-58443bc87532",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1c525-6820-4739-bd00-40f0c6a17d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedGroupKFold "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9973a1-5b4e-4f36-a19b-5f52be25d99d",
   "metadata": {},
   "source": [
    "## Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8be1ae-d453-4cb9-8fa3-ceb51cacba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559fd68-cc29-490e-af8a-00dfa62bc8b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d1222-c1fa-4899-b26e-ec674f3a4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with Path(filename).open(encoding='utf8') as handle:\n",
    "        ann = json.load(handle)\n",
    "    return ann    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf9e2a-6d8a-4b7d-9d60-e911c83bbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../input/data/'\n",
    "path_ICDAR17 = 'ICDAR17_Korean/ufo/'\n",
    "data_nm = 'train.json'\n",
    "\n",
    "data_org = read_json(os.path.join(root_dir, path_ICDAR17, data_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfd3aa-bfff-48ed-ada7-912803d84e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_org['images']['img_4380.jpg']['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de625613-4351-4419-b6d5-88f4ac859b24",
   "metadata": {},
   "source": [
    "# Get image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58ccda-4791-4163-b754-e86e44108893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_size(quads):\n",
    "    \"\"\" 단어 영역의 사각형 좌표가 주어졌을 때 가로, 세로길이를 계산해주는 함수.\n",
    "    TODO: 각 변의 길이를 단순히 max로 처리하기때문에 직사각형에 가까운 형태가 아니면 약간 왜곡이 있다.\n",
    "    Args:\n",
    "        quads: np.ndarray(n, 4, 2) n개 단어 bounding-box의 4개 점 좌표 (단위 pixel)\n",
    "    Return:\n",
    "        sizes: np.ndarray(n, 2) n개 box의 (height, width)쌍\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    for i, j in [(1, 2), (3, 0), (0, 1), (2, 3)]: # [right(height), left(height), upper(width), lower(width)] sides\n",
    "        dists.append(np.linalg.norm(quads[:, i] - quads[:, j], ord=2, axis=1))\n",
    "\n",
    "    dists = np.stack(dists, axis=-1).reshape(-1, 2, 2) # shape (n, 2, 2) widths, heights into separate dim\n",
    "    return np.rint(dists.mean(axis=-1)).astype(int)\n",
    "\n",
    "\n",
    "def rectify_poly(poly, direction, img_w, img_h):\n",
    "    \"\"\"일반 polygon형태인 라벨을 크롭하고 rectify해주는 함수.\n",
    "    Args:\n",
    "        poly: np.ndarray(2n+4, 2) (where n>0), 4, 6, 8\n",
    "        image: np.ndarray opencv 포멧의 이미지\n",
    "        direction: 글자의 읽는 방향과 진행 방향의 수평(Horizontal) 혹은 수직(Vertical) 여부\n",
    "    Return:\n",
    "        rectified: np.ndarray(2, ?) rectify된 단어 bbox의 사이즈.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_pts = poly.shape[0]\n",
    "    assert n_pts % 2 == 0\n",
    "    if n_pts == 4:\n",
    "        size = get_box_size(poly[None])\n",
    "        h = size[:, 0] / img_h\n",
    "        w = size[:, 1] / img_w\n",
    "        return np.stack((h,w))\n",
    "\n",
    "    def unroll(indices):\n",
    "        return list(zip(indices[:-1], indices[1:]))\n",
    "\n",
    "    # polygon하나를 인접한 사각형 여러개로 쪼갠다.\n",
    "    indices = list(range(n_pts))\n",
    "    if direction == 'Horizontal':\n",
    "        upper_pts = unroll(indices[:n_pts // 2]) # (0, 1), (1, 2), ... (4, 5)\n",
    "        lower_pts = unroll(indices[n_pts // 2:])[::-1] # (8, 9), (7, 8), ... (6, 7)\n",
    "\n",
    "        quads = np.stack([poly[[i, j, k, l]] for (i, j), (k, l) in zip(upper_pts, lower_pts)])\n",
    "    else:\n",
    "        right_pts = unroll(indices[1:n_pts // 2 + 1]) # (1, 2), (2, 3), ... (4, 5)\n",
    "        left_pts = unroll([0] + indices[:n_pts // 2:-1]) # (0, 9), (9, 8), ... (7, 6)\n",
    "\n",
    "        quads = np.stack([poly[[i, j, k, l]] for (j, k), (i, l) in zip(right_pts, left_pts)])\n",
    "\n",
    "    sizes = get_box_size(quads)\n",
    "    if direction == 'Horizontal':\n",
    "        h = sizes[:, 0].max() / img_h\n",
    "        widths = sizes[:, 1]\n",
    "        w = np.sum(widths) / img_w\n",
    "        return np.stack((h,w)).reshape(2,-1)\n",
    "        #return np.stack((h,w))\n",
    "    elif direction == 'Vertical':\n",
    "        heights = sizes[:, 0]\n",
    "        w = sizes[:, 1].max() / img_w\n",
    "        h = np.sum(heights) / img_h\n",
    "        return np.stack((h,w)).reshape(2,-1)\n",
    "    else:\n",
    "        h = sizes[:, 0] / img_h\n",
    "        w = sizes[:, 1] / img_w\n",
    "        return np.stack((h,w),-1)\n",
    "    \n",
    "def get_image_dfs(data):\n",
    "    df = {}\n",
    "    df['image'] = []\n",
    "    df['word_counts'] = []\n",
    "    df['image_width'] = []\n",
    "    df['image_height'] = []\n",
    "    df['image_tags'] = []\n",
    "    img_tags = []\n",
    "\n",
    "    quads = []\n",
    "    polys = []\n",
    "    seq_length = []\n",
    "    hor_sizes = []\n",
    "    ver_sizes = []\n",
    "    irr_sizes = []\n",
    "    languages = []\n",
    "    orientation = []\n",
    "    word_tags = []\n",
    "    aspect_ratio = []\n",
    "    ver_string = []\n",
    "\n",
    "    bbox_properties = []\n",
    "    \n",
    "    for image_key, image_value in data[\"images\"].items():\n",
    "        df['image'].append(image_key)\n",
    "        img_w = image_value['img_w']\n",
    "        img_h = image_value['img_h']\n",
    "        df['image_width'].append(img_w)\n",
    "        df['image_height'].append(img_h)\n",
    "        df['image_tags'].append(image_value['tags'])\n",
    "        df['image_tags']= [['None'] if v is None else v for v in df['image_tags']] # our data does not inlcude multi-tag images \n",
    "        word_ann = image_value['words']\n",
    "        count_ill = 0 \n",
    "        for word in word_ann.values():\n",
    "            if word['illegibility']== False:\n",
    "                orientation.append(word['orientation'])\n",
    "                orientation = [v for v in orientation]\n",
    "                seq_length.append(len(word['transcription']))\n",
    "                languages.append(word['language'])\n",
    "                languages = [['None'] if v is None else v for v in languages] # our data does not inlcude multi-language words\n",
    "                if word['word_tags'] != None:\n",
    "                    word_tags.extend(word['word_tags'][:])\n",
    "                elif word['word_tags']== None:\n",
    "                    word_tags.append('None')\n",
    "                poly = np.int32(word['points'])\n",
    "                size = rectify_poly(poly, word['orientation'], img_w, img_h)\n",
    "                if word['orientation'] == 'Horizontal':\n",
    "                    hor_sizes.append(size)\n",
    "                    bbox_properties.append([image_key, size, 'Horizontal'])\n",
    "                    # print(image_key, size, 'Horizontal')\n",
    "                elif word['orientation'] == 'Vertical':\n",
    "                    ver_sizes.append(size)\n",
    "                    bbox_properties.append([image_key, size, 'Vertical'])\n",
    "                    # print(image_key, size, 'Vertical')\n",
    "                else:\n",
    "                    irr_sizes.append(size)\n",
    "                    bbox_properties.append([image_key, size, 'Irregular'])\n",
    "            else:\n",
    "                count_ill += 1\n",
    "\n",
    "        df['word_counts'].append(len(word_ann)-count_ill)\n",
    "\n",
    "\n",
    "    all_sizes = hor_sizes + ver_sizes + irr_sizes\n",
    "    quad_area = [all_sizes[i][0]*all_sizes[i][1] for i in range(len(all_sizes))]\n",
    "    total_area = []\n",
    "    for s in quad_area:\n",
    "        if s.shape[0] == 1:\n",
    "            total_area.append(np.sum(s[0])) \n",
    "        else:\n",
    "            total_area.append(np.sum(s))\n",
    "\n",
    "    hor_aspect_ratio = [hor_sizes[i][1]/hor_sizes[i][0] for i in range(len(hor_sizes))]\n",
    "    ver_aspect_ratio = [ver_sizes[i][1]/ver_sizes[i][0] for i in range(len(ver_sizes))]\n",
    "\n",
    "    image_df = pd.DataFrame.from_dict(df)\n",
    "    bbox_df = pd.DataFrame(data=bbox_properties,\n",
    "                          columns=['image', 'size', 'orientation'])\n",
    "    \n",
    "    bbox_df['aspect_ratio'] = bbox_df.apply(lambda x: (x['size'][1]/x['size'][0])[0], axis=1)\n",
    "    \n",
    "    return image_df, bbox_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95635d4-afbd-4be5-9ba5-354586020fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df, bbox_df = get_image_dfs(data_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299ddbd-fec0-4e45-8d00-2b1144cf7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image data frame\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dde0a-107e-4b20-9be4-f1a8df6827b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check bbox dataframe\n",
    "bbox_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176996a-8d3d-40fe-9209-3253cedcddcc",
   "metadata": {},
   "source": [
    "# Get validation version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af8d89-a736-4c64-bd76-c25882c10137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split images at random\n",
    "X_train_v1, X_valid_v1, y_train_v1, y_valid_v1 = \\\n",
    "train_test_split(image_df.image, image_df.image, test_size=0.2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3cc598-dd45-45e6-b3ea-004af6d5fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple check split result\n",
    "print(sum(X_train_v1!=y_train_v1))\n",
    "print(set(y_train_v1).intersection(set(y_valid_v1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e7ada-6ef1-4e65-a27f-82deebc180bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_org['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6054358-60be-4f9e-a82d-1f8ad9890973",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v1 = {'images': {k: v for k, v in data_org['images'].items() if k in X_train_v1.values}}\n",
    "valid_v1 = {'images': {k: v for k, v in data_org['images'].items() if k in X_valid_v1.values}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25067fe1-4040-424e-99d7-8d03dceb2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for check\n",
    "print(len(train_v1['images']))\n",
    "print(len(valid_v1['images']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d8144-d458-4c25-80b9-c23cefcb5541",
   "metadata": {},
   "source": [
    "# Save validation version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb829f68-1469-4786-ad3a-829ba3304e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data: dict, file_nm: str, dir_path: str):\n",
    "    with open(os.path.join(dir_path, file_nm), 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c0d63-7b24-4435-b599-027512f59297",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [train_v1,\n",
    "             valid_v1\n",
    "            ]\n",
    "file_nm_list = ['train_v1.json',\n",
    "                'valid_v1.json',\n",
    "               ]\n",
    "\n",
    "for data, file_nm in zip(data_list, file_nm_list):\n",
    "    save_json(data, file_nm, dir_path=os.path.join(root_dir, path_ICDAR17))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6d720-936c-494d-8f29-4b5d71ac63e5",
   "metadata": {},
   "source": [
    "# Get validation version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec95c73-5b07-4b65-8e0e-80b59806be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - Check aspect ratio distribution\n",
    "fig_aspect_ratio, ax_aspect_ratio = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "sns.histplot(bbox_df.aspect_ratio, ax=ax_aspect_ratio[0, 0])\n",
    "ax_aspect_ratio[0, 0].set_title('Basic distribution', fontsize=25)\n",
    "\n",
    "sns.boxplot(bbox_df.aspect_ratio, ax=ax_aspect_ratio[1, 0])\n",
    "\n",
    "sns.distplot(np.log(bbox_df.aspect_ratio), ax=ax_aspect_ratio[0, 1])\n",
    "ax_aspect_ratio[0, 1].set_title('Log transformed distribution', fontsize=25)\n",
    "\n",
    "sns.boxplot(np.log(bbox_df.aspect_ratio), ax=ax_aspect_ratio[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b125b1-6d5d-4247-a131-862a6db91698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_binning(data:pd.Series, min_elements=10, log_transform=True, print_outlier_percent=False):\n",
    "    # The goal of this function is binning a continuous-valued 1-d sequence for stratified splitting\n",
    "    \"\"\"\n",
    "    Approximate the distribution of continuous 1-d sequence as a discrete distribution by pandas cut function\n",
    "\n",
    "    Args:\n",
    "        data: Input 1-d sequence as a pandas format.\n",
    "        min_elements: The number of miminum elements for a bin.\n",
    "                      It is used for finding moderate number of bins.\n",
    "        log_transform: Whether using natural log transformation before binning or not.\n",
    "        print_outlier_percent: Whether printing outlier ratio among whole dataset or not.\n",
    "\n",
    "    Returns:\n",
    "        1-d pandas series with discrete class\n",
    "    \"\"\"   \n",
    "    assert type(data) is pd.Series\n",
    "    \n",
    "    def bind_outlier(data: pd.Series, print_outlier_percent=False):\n",
    "        \"\"\"\n",
    "        Outlier binding function for 1-d array.\n",
    "        Outlier is determined by statistical convention using 25 & 75 percentile and IQR \n",
    "\n",
    "        Args:\n",
    "            data: Input 1-d sequence as a pandas format.\n",
    "            print_outlier_percent: Whether printing outlier ratio among whole dataset or not.\n",
    "\n",
    "        Returns:\n",
    "            It returns the outlier binded 1-d array.\n",
    "        \"\"\"    \n",
    "        assert type(data) is pd.Series\n",
    "\n",
    "        cnt_outlier = 0\n",
    "\n",
    "        q1, q3 = np.percentile(data, 25), np.percentile(data, 75)\n",
    "        lower = q1 - 1.5*(q3-q1)\n",
    "        upper = q3 + 1.5*(q3-q1)\n",
    "\n",
    "        new_data = deepcopy(data)\n",
    "\n",
    "        cnt_outlier += len(new_data[new_data <= lower])\n",
    "        new_data[new_data <= lower] = lower\n",
    "\n",
    "        cnt_outlier += len(new_data[new_data >= upper])\n",
    "        new_data[new_data >= upper] = upper\n",
    "\n",
    "        if print_outlier_percent:\n",
    "            print(f'Outliers account for [{100*cnt_outlier/len(data):.3f}]% of total data')\n",
    "\n",
    "        return new_data\n",
    "    \n",
    "    # binding outlier\n",
    "    if log_transform:\n",
    "        binded_data = bind_outlier(np.log(data), print_outlier_percent)\n",
    "    else:\n",
    "        binded_data = bind_outlier(data, print_outlier_percent)\n",
    "        \n",
    "    num_bins = 10\n",
    "    max_iter_limit = 9999\n",
    "    iter_cnt = 0\n",
    "    while True:\n",
    "        dist_approx = pd.cut(binded_data, bins=num_bins, labels=np.arange(num_bins))\n",
    "        min_elem_cnt = dist_approx.value_counts().min() # Miminum number of elements for a bin\n",
    "        \n",
    "        if min_elem_cnt < min_elements:\n",
    "            print(f'Minimum # of elements for a bin is [{min_elem_cnt}]')\n",
    "            print(f'Number of bins [{num_bins}]')\n",
    "            break\n",
    "        \n",
    "        if iter_cnt > max_iter_limit:\n",
    "            print('Iteration exceeded limit')\n",
    "            raise RuntimeError\n",
    "        \n",
    "        num_bins += 1\n",
    "        iter_cnt += 1\n",
    "    \n",
    "    return dist_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ce367-71ce-4c73-8b61-025e45c8e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auto_binning = auto_binning(bbox_df.aspect_ratio, print_outlier_percent=True)\n",
    "sns.displot(test_auto_binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ab019-c2ea-456e-8c86-b0a250eaffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df['aspect_class'] = auto_binning(bbox_df.aspect_ratio, print_outlier_percent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0768f-58f1-4903-8e06-4e0c8e1e9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_val_v2 = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "for train_idx_v2, valid_idx_v2 in cv_val_v2.split(bbox_df.aspect_class, bbox_df.aspect_class, bbox_df.image): \n",
    "    pass\n",
    "    \n",
    "print(f'Length train idx [{len(train_idx_v2)}]')\n",
    "print(f'Length valid idx [{len(valid_idx_v2)}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59d222-d0c1-46e5-ba73-a2b1e91a4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device images by aspect ratio class\n",
    "train_image_v2 = set(bbox_df.image[train_idx_v2])\n",
    "valid_image_v2 = set(bbox_df.image[valid_idx_v2])\n",
    "\n",
    "# Check v2 exclusivity\n",
    "print(f'Intersection between train images v2 & valid images v2 : [{train_image_v2.intersection(valid_image_v2)}]')\n",
    "\n",
    "train_image_v2 = list(train_image_v2)\n",
    "valid_image_v2 = list(valid_image_v2)\n",
    "\n",
    "train_v2 = {'images': {k: v for k, v in data_org['images'].items() if k in train_image_v2}}\n",
    "valid_v2 = {'images': {k: v for k, v in data_org['images'].items() if k in valid_image_v2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49799ad-3b8e-49fc-a0db-cfa155a3cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length train v2 [{len(train_v2['images'])}] || valid v2 [{len(valid_v2['images'])}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6d08b-eab9-418c-be28-ebe72e647c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - version 2 aspect class distribution check\n",
    "plt.bar(bbox_df.aspect_class[train_idx_v2].value_counts().sort_index().index,\n",
    "        bbox_df.aspect_class[train_idx_v2].value_counts().sort_index(),\n",
    "        label='train')\n",
    "plt.bar(bbox_df.aspect_class[valid_idx_v2].value_counts().sort_index().index,\n",
    "        bbox_df.aspect_class[valid_idx_v2].value_counts().sort_index(),\n",
    "        color='orange', label='valid')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35a47c-600e-4243-a700-cc86cc514437",
   "metadata": {},
   "source": [
    "# Save validation version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484046b-2f48-4912-b561-05303e354a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_v2 = [train_v2,\n",
    "             valid_v2\n",
    "            ]\n",
    "file_nm_list_v2 = ['train_v2.json',\n",
    "                'valid_v2.json',\n",
    "               ]\n",
    "\n",
    "for data, file_nm in zip(data_list_v2, file_nm_list_v2):\n",
    "    save_json(data, file_nm, dir_path=os.path.join(root_dir, path_ICDAR17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29d1f0-a55f-47b6-b2cf-1b927edcbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_v2 = read_json(os.path.join(root_dir, path_ICDAR17, 'train_v2.json'))\n",
    "data_test_v2['images'][random.sample(data_test_v2['images'].keys(), 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d456d-29fb-4a28-9007-a5673b64b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_v2 = read_json(os.path.join(root_dir, path_ICDAR17, 'valid_v2.json'))\n",
    "data_test_v2['images'][random.sample(data_test_v2['images'].keys(), 1)[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
